---
title: "Clasificación doctech"
author: "Luis Daniel Chavarría"
date: "20/11/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clasificación de causas de rechazo de documentos técnicos

El objetivo de este trabajo es clasificar los planos de acuerdo a sus motivos de rechazo en 3 categorías: 

- Forma: El plano tiene errores en la distribución, cajetines, faltas en símbolos, entre otros.
- Rigor técnico: El plano tiene errores técnicos graves o faltas de especificaciones, por lo que debe ser revisado a profundidad.
- Procedimiento: El plano fue enviado a circuito de firmas de manera incorrecta, por ejemplo, cuando no se citan los pool de firmantes necesarios o cuando se cita a un validador que no está capacitado para hacerlo.

## El método

Julia Silge en su [viñeta](https://www.r-bloggers.com/text-classification-with-tidy-data-principles/) propone el modelo de regresión  con regularización LASSO para la clasificación basada en palabras y obtienen resultados muy interesantes realizando clasificaciones adecuadas para dos libros: La guerra de los mundos vs Orgullo y prejuicio.

# Librerías

```{r}
library(tidyverse)
library(glmnet)
library(tidytext)
library(data.table)

raw_verbatim <- fread("consolidado_ph2.csv") %>% as_tibble()
planos <- fread("planos_pt2.csv") %>% as_tibble() %>% 
  mutate(PIE_numero = str_trim(PIE_numero))
```

# Data cleaning

```{r}
clean_verbatim <- raw_verbatim %>% 
  filter(Maturity == "Refused") %>% 
  inner_join(planos, by = c("Name/Id" = "DOCTECH_numero"))

clean_verbatim %>% 
  fwrite("test.csv", sep = ";")
```


```{r}
tidy_words <- clean_verbatim %>% 
  select(`Name/Id`, Comentarios, ELTDEC_numero, 
         ELTDEC_designation, FCT_numero, FCT_designation, 
         ESO_num_PG, ESO_designation_PG, PIE_numero, 
         PIE_designation, PIEIND_Dernier_indice, PIEIND_typologie) %>% 
  mutate(Comentarios = str_remove_all(Comentarios, "\n")) %>% 
  unnest_tokens(word, Comentarios) %>%
  group_by(word) %>% 
  filter(n() > 10) %>% 
  ungroup()
```


```{r}
tidy_words %>% 
  count(FCT_designation, word, sort = T) %>% 
  anti_join(get_stopwords(language = "en")) %>% 
  anti_join(get_stopwords(language = "fr")) %>% 
  anti_join(get_stopwords(language = "pt")) %>% 
  anti_join(get_stopwords(language = "es")) %>% view()
```

