---
title: "Clasificación doctech"
author: "Luis Daniel Chavarría"
date: "20/11/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Clasificación de causas de rechazo de documentos técnicos

El objetivo de este trabajo es clasificar los planos de acuerdo a sus motivos de rechazo en 3 categorías: 

- Forma: El plano tiene errores en la distribución, cajetines, faltas en símbolos, entre otros.
- Rigor técnico: El plano tiene errores técnicos graves o faltas de especificaciones, por lo que debe ser revisado a profundidad.
- Procedimiento: El plano fue enviado a circuito de firmas de manera incorrecta, por ejemplo, cuando no se citan los pool de firmantes necesarios o cuando se cita a un validador que no está capacitado para hacerlo.

## El método

Julia Silge en su [viñeta](https://www.r-bloggers.com/text-classification-with-tidy-data-principles/) propone el modelo de regresión  con regularización LASSO para la clasificación basada en palabras y obtienen resultados muy interesantes realizando clasificaciones adecuadas para dos libros: La guerra de los mundos vs Orgullo y prejuicio.

# Librerías

```{r}
library(tidyverse)
library(glmnet)
library(tidytext)
library(data.table)

raw_verbatim <- fread("consolidado_ph2.csv") %>% as_tibble()
planos <- fread("planos_pt2.csv", encoding = "UTF-8") %>% as_tibble() %>% 
  mutate(PIE_numero = str_trim(PIE_numero))
```

# Data cleaning

```{r}
clean_verbatim <- raw_verbatim %>% 
  filter(Maturity == "Refused") %>% 
  inner_join(planos, by = c("Name/Id" = "DOCTECH_numero")) %>% 
  mutate(Comentarios = str_trim(Comentarios))
```

# Stratified Sampling

¿Cuáles perímetros están menos representados? 

```{r}
clean_verbatim %>% 
  count(ELTDEC_numero, sort = T) %>% 
  mutate(ELTDEC_numero = as_factor(ELTDEC_numero)) %>% 
  ggplot(aes(x = ELTDEC_numero, y = n)) +
  geom_col() +
  scale_y_log10()
```

Los perímetros B4, B1, 55, 56 Y 94 cuentan con menos de 10 observaciones. Se excluirán para el muestreo aleatorio y se agregarán posteriormente con su total de observaciones.

```{r}
set.seed(1)
rare_eltdec <- clean_verbatim %>% 
  filter(ELTDEC_numero == "B4" | ELTDEC_numero == "B1" | ELTDEC_numero == "55" | ELTDEC_numero == "56" | ELTDEC_numero == "94")


muestra <- clean_verbatim %>% 
  anti_join(rare_eltdec, by = "Name/Id") %>%
  filter(Comentarios != "") %>%
  group_by(ELTDEC_numero) %>% 
  sample_n(size = 15) %>%
  ungroup() %>% 
  bind_rows(rare_eltdec)

# write_excel_csv2(muestra, "muestra.csv")
```

```{r}
train_set <- fread("train_set.csv") %>% as_tibble()
```


# Modelling

```{r}
tidy_words <- train_set %>% 
  select(`Name/Id`, Comentarios, ELTDEC_numero, 
         ELTDEC_designation, FCT_numero, FCT_designation, 
         ESO_num_PG, ESO_designation_PG, PIE_numero, 
         PIE_designation, PIEIND_Dernier_indice, PIEIND_typologie, causa_rechazo) %>% 
  mutate(Comentarios = str_replace_all(Comentarios, "\n", " ")) %>% 
  unnest_tokens(word, Comentarios) %>%
  group_by(word) %>% 
  filter(n() > 10) %>% 
  ungroup()
```


```{r}
words_frequency <- tidy_words %>%
  filter(causa_rechazo != "") %>% 
  count(causa_rechazo, word, sort = T) %>% 
  anti_join(get_stopwords(language = "en"), by = "word") %>% 
  anti_join(get_stopwords(language = "fr"), by = "word") %>% 
  anti_join(get_stopwords(language = "pt"), by = "word") %>% 
  anti_join(get_stopwords(language = "es"), by = "word")

words_frequency %>%
  filter(n > 1) %>% 
  ggplot(aes(reorder_within(word, n, causa_rechazo), n, fill = causa_rechazo)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~causa_rechazo, scales = "free") + 
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = NULL, y = "Palabras", title = "Palabras más comunes después de remover stop words")

ggsave("frequency.png", units = "mm", width = 215, height = 279)
```

